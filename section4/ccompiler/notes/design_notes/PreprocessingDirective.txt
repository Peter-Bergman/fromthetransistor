I need to decide what kind of parser I am going to use to process directives.

I will do something along one of the following lines:
 1. Make some kind of transformer from Parser String type to 
	GenParser String SymbolTable [String] where SymbolTable is some
	kind of Dictionary with Get and Set operations.
 2. Flatten out list of Strings returned from lexer, then use 
	CharParser SymbolTable String when processing directives.


I strongly gravitate toward 2 more. It seems like the code would be cleaner.
I don't want to deal with skipping tons of whitespace.
I don't want to have to intercalate.
Plus, I think it will help me understand Parsec better if I try using GenParser.
Actually, intercalating wouldn't even be much of a hastle.
And skipping spaces is not necessarily worse than using function constructors.

I looked through a paper I read a while ago.
It's called "Monadic Parser Combinators" by Graham Hutton and Erik Meijer. 
I wanted to look at their definition of the sat parser again.
I think it could help me create some kind of transformer from 
	Parser String to a desired GenParser.
In the paper, sat is defined as follows.

sat :: (Char -> Bool) -> Parser Char
sat p = item 'bind' \x ->
	if p x then result x else zero

In my case I would need a similar type signature.
stringSat :: (String -> Bool) -> Parser String
or
stringSat :: (String -> Bool) -> Parser [String]

Realistically, I could simply use substitution on the sat definition.
I would have to make sure that I have an implementation of item.
I think it would be helpful to have some kind of function
stringParserToStringChecker :: Parser String -> (String -> Bool)
I could use that function to create input for stringSat.

stringParserToStringChecker :: Parser String -> (String -> Bool)
stringParserToStringChecker parser inputString = 
	case (parsedString) of
		Left err -> False
		Right xs -> xs == inputString
	where
		parsedString = parse parser "" inputString

works

I'm using stringParserToStringChecker now in the ParserStringTransformer module.

I just found the getState, setState, and updateState functions in ParsecPrim.

I now need some definition for stringSat.
This could work, I think.
stringSat stringToBool = item >>= \x ->
	if stringToBool x then return x else fail ""

item = do
	input <- getInput
	let nextToken = head input
	return nextToken
	could work as an implementation of item
return nextToken could work as an implementation of item.

I am reading through the paper "Parsec, a fast combinator parser" by Daan Leijen.
I think I'm going to scratch the definition of item above.
I have found the token and tokenPrim GenParser constructors in ParsecPrim.
I don't think that my above definition will update the source position.


item = tokenPrim show nextPos testString
	where
		nextPos pos x xs = updatePosString pos x
		testString x = true

Me likey.
I will try compiling and testing this tomorrow.

There have been a lot of changes since the last time I touched this document.
I cannot process preprocessing directives within the parser code.
The reason for this is the include directive.
The include directive requires IO functionality.

I made lots of changes to the parsers in the Preprocessor.
All of the parsers used in ControlLine.hs now parse ADT's from the AST
in AbstractSyntaxTree.hs.
The same with TextLine.hs

I need to refactor the string.*Satisfy.? parser constructors in 
    PreprocessingParser.hs
They need to be able to handle my changes to the Lexer.
The Lexer no longer skips horizontal spacing.
Rather, it lexes--the word for next to each other
What is it? What is it? What is it? Next to each other.
Not immediately? Not continuous? What is the word that 
    I am looking for? It lexes, not continuous
Not contiguous.
Not serial.
What is that word?
It's on the tip of my tongue.
Not immediately sequenced, though that would
    convey the same meaning.
God fucking dammit (no, no, we don't say that; yes we do)
What is the word?
I am going to stop blasting the keyboard to think without 
    continuous writing.
CONSECUTIVE!!!!!
That's the word! Yes!
The lexer lexes consecutive horizontal spacing characters into
    a single token.

Next session. Didn't get super far last time.
Spent most of the time scratching my head metaphorically.
As I said before, I need to have the string.*Satisfy.?
    parser builders handle the changes to the lexer.
They need to be able to easily parse whitespace before them.
The first change that I would like to make is actually an
    action to clean up and abstract the code in the 
    PreprocessingParser.hs module.
I would like there to be one and only one function in that
    module that calls tokenPrim from Text.Parsec.Prim.
Then, I would like all of the string.*Satisfy.? functions
    to have that function in their expression tree.

Third session, here goes.
Okay, I need to decide what function will directly
    call tokenPrim.
In order for all of the other string satisfy functions
    to have this function in their expression trees,
    the function that directly calls tokenPrim will
    have to be stringSatisfyT, or at least a function
    that is abstracted to construct any
    PreprocessingParserX t, like stringSatisfyT.
There are three pathways that I can take from here:
1. Have the current string.*Satisfy functions
    optionally skip horizontal spacing.

Fourth session, continuing on...
2. Have the current string.*Satisfy functions
    not consume spacing and write a new function
    (or functions) that does consume spacing.
3. Add another parameter to all of the 
    string.*Satisfy.? functions.
The third option does not require writing ANY new functions.
Thus, no function identifiers in the Preprocessor 
    directory would need to be modified.
However, every call to a string.*Satisfy.? function
    would have to be modified to pass the new parameter.
I do not know yet where in the parameter lists I would
    like to add the ...

Session 5, continuing on:
... new parameter; the start or the back?
One position may have advantages of simplicity
    in the current code invoking those functions.
I will now talk about the first option, though.
If I opted for the first method, I would have
    to write a new function that LParen would
    call.
My only concern with the first option is a
    lack of semantic clarity.
If a parser is going to skip preceding
    horizontal white space, then it would
    be nice to have that be explicit in
    the parser's name.
Maybe not.

Session 6
Well, I guess it would be fine.
Changing the name of a function identifier,
    I think, would be more convenient than
    adding a parameter, as in the 3rd
    option.
I say this because of partial applications
    and the weirdness that comes with that.
By adding a parameter to a function, I may
    kill some of the sweet and sexy
    syntactic sugar used in one of the
    invoking modules.
That wouldn't be the end of the world,
    but it would take more time.
The commit changes would be easier to
    read and comprehend if I change
    a function name than if I change
    the function call syntax used
    somewhere.
Okay, then, I don't like the third option.
The third option is ruled out.

Session 7
What about the second option.
It is barely on screen for me.
This one is not dissimilar from the
    first.
With either of these options, I will
    have to change the function
    identifiers all throughout the
    Preprocessor directory.
Well, that is not actually the
    case.
The first one, if I don't change
    the name of the functions
    currently in use, will only
    require further changes in
    LParen, I think.
Maybe LParen.hs and the DefineDirective.hs
    module that imports LParen (the
    only LParen import, I believe).
Let me summarize the remaining options
    to myself.

Session 8
I just realized that option 1 is the same
    as option 3.
Now, I'm confused as to what I thought
    option 1 was when I decided it was
    better than option 3.
I will redefine option 1 here:
1. Have the current string.*Satisfy.? functions
    consume horizontal spacing, and make them
    all derive their definition from a new
    function that does not consume preceding
    horizontal whitespace.
This is as opposed to making the current
    string satisfy functions not consume
    preceding white space and writing
    functions that do.
The second option would require me to
    rewrite all...

Session 9
... --I'll start that sentence over.
The second option would require me to
    write a new version of each 
    string satisfy function.
With the first option, I could
    simply write one new function
    and have stringSatisfyT
    invoke it.
Is that not correct?
Yes, it is.
Okay, then first option it is.
That leaves one last question.
Will I change the name of the
    current string.*Satisfy.?
    functions to make explicit
    that they skip preceding
    whitespace?


Session 10
I don't want to.
Well, I was going to ask myself
    if changing the names would
    probably lead to payoffs in
    easier to read code.
It could.
However, I am the only person
    working on this repository.
I do not need to make this design
    decision now.
It will probably be in the side
    of my mind as I write the
    remainder of the parsers.
That will give plenty of
    oppurtunities for the
    OCD parts of me to speak
    up.
There is no need for me to
    demand that I make a
    decision now.
One of my favorite principles
    for making design decisions
    is to push them further
    and further down the line,
    if possible.

I am going to go with option 1.

I need to build a parser for postfix expressions.
It is difficult to do so without making one that will infinitely recurse.
My current idea is to define a parser called primitivePostfixExpression.
It will parse the postfix expression mutations with a typename and 
    initializer list or a primary expression.
Then, I would feed the parsed output, a postfix expression, to a parser
    constructor that parses possibly following parts of the postfix
    expression.
The constructed parser would return the initially parsed postfix expression
    if a more complex postfix expression could not be parsed.
The constructed parser, should, therefore, never fail.
The parser constructors for non-primitive postfix expressions are not
    semantically correct in this way of doing parsing postfix expressions.
I would either like to figure out better parser constructor names or use a
    more semantically correct design.


I need to be able to parse integer constants for the Preprocessor.
I am writing that parser inside of ConstantExpression.hs, though
    I guess constants could be in their own module.
Anyways, some of the syntactical structures that make up an integer
    constant are more fine grained than a single string token
    output by the lexer.
An example of this is hexadecimal-digit (as in the ISO document),
    HexadecimalDigit in the AST. I could double check that, but I
    don't think it is actually necessary in order for me to move on.
Anyways, hexadecimal digits are single characters. There could me
    many of them in a single token.
I think my solution for this problem is to transform Char
    token parsers into PreprocessingParserX parsers. I wrote out
    in my notebook what I intuitively think the type signature of
    that transformation is. Checking.
transformerName :: Text.Parsec.String.Parser t ->
    PreprocessingParserX t
That is actually a much simpler signature compared to a lot of the
    other transformer functions that I've written that map char
    token parsers to PreprocessingParserX parsers. Most of those
    other parsers (if not all) require some kind of first class
    function to be passed as an argument to transform a string into
    type t, and the input parser does not have type t, I believe,
    for any of the previous parser transformers that I've written.
I think I will copy or abstract some of the code in
    PreprocessingParser.hs for this transformation, specifically
    the code that does the case expression for the parsing error
    and successful parse, the case expression that handles
    some Either data type parameter.
I will look in the PreprocessingParser module now for that code.
I think I will be using or abstracting the code for the function
    stringParserToStringChecker.
Below is the signature.
stringParserToStringChecker :: Parser String -> (String -> Bool)
I think I will be copying, rather than abstracting this code.
    I do not want a function that returns a bool.
Rather, I need the parsed AST.
I can reuse the pattern matches for the case expression and the
    where definition.
I need a name for the parser transformer that I am trying to make.
I could call it charTokenParserToStringTokenParser.
That is a long name, but I don't know of any better way to
    describe what the function does.
A more concise way to say that is charToStringTokenParser.
I will use that for now.
Well, actually I don't want to use that name.
It does not output just any string token parser, rather a stateless
    one (based on the current definition of PreprocessingParserX)
I will figure out a better name after implementing the function.
Hold on! I need to rethink the type signature.
I don't think I can implement what I need with the current
    signature.
I think I will need a string (the token being parsed) as an input.
I think I've got it. I wrote some extra code in the where
    clause that makes charToStringTokenParser fail if it does not
    parse the entire string token.
If charToStringTokenParser throws a ParseError, there are two
    parser positions printed, which could be confusing. I wish I
    could grab the messages from a ParseError. I think I can.
    I looked up the definition of ParseError.
I was trying to deconstruct ParseError myself. I could not
    because the constructor was not exported.
I just found the function, also in Text.Parsec.Error, errorMessages.
The errorMessages returns the error messages of a ParseError.
I will use that to rethrow the parse error.
Appartently, the Message data type does not derive the Show
    typeclass. That is fine.
There is a function messageString that returns the string for
    the message.
I will use that instead of show to pass the messages from the
    internal ParseError to parserFail.
I think I will have to reduce a list of messages.
I did that. The messages do not have the Expected or Unexpected
    prefix before them.
I can figure that out in the future.
Oh, this problem can be solved by the showErrorMessages function
    in the Text.Parsec.Error module.
Furthermore, I can use the ending of the ParseError instantiation
    of the Show typeclass.
See the source code for instance Show ParseError.

I will work on the showing embedded ParseError objects in a little bit.
Before fucking around with the diagnostic messages, I want to confirm that
    I can convert any char token parser to a PreprocessingParserX type
    parser.
I want to implement hexadecimalConstant in this way.
I need to think of a way to name the Parser HexadecimalConstant
    and a different name for the PreprocessingParserX HexadecimalConstant.


I have an issue with the lexer.
When I lex the character constant "L'a'", the character constant is split
    into two tokens, shown in a haskell string on the line below.
["L","'a'"]
That is not what I expect.
I expected it to be just one token.
I checked the iso c document, and I looked under the syntactic definition
    of preprocessing-token.
One of the possible syntaxes for a preprocessing-token is a simple
    character-constant.
Therefore, I think I messed up the order of alternatives in my
    preprocessing-token parser implementation.
The identifier parser is earlier in the sequence of alternatives.
I think that the L is being parsed as an identifier, and the 'a' is
    parsed as something else.
I will see which of the alternative parsers parse the full string "'a'".
Maybe headerName?
Maybe stringLiteral, but I bet that only messes with double quotes.
I'll try now.
The headerName parser did not consume the first '\'' character.
The same happened with the stringLiteral parser.
I will try each of the alternatives.
Oh, I just realized that the string "'a'" will itself be parsed as a
    character constant, even without the 'L' prefix.
That makes more sense.
I will try parsing the string "'a'" with ppNumber.
The ppNumber parser is not exported from the Lexer.PreprocessingToken
    module.
I added it to the exports.
I reloaded my modules in ghci, and I tried parsing "'a'"with ppNumber, and
    it failed on the first character.
I moved characterConstant before identifier (past ppNumber and identifier)
    in the order of alternatives for the preprocessingToken parser.
Now, it is only behind headerName.
None of the possible first characters of a header-name can be the first
    character of a character-constant, and vise versa.
I think that I solved the problem without creating any new ones.
I still need to test the preprocessingToken parser on the string "L'a'" to
    find out.
The preprocessingToken parsed the entire string "L'a'".
That is good.
I will now check that the lexer lexes that string in the context of other
    strings properly.
It seemed to work properly.
I'm calling this issue fixed. I will commit the changes to
    PreprocessingToken.hs.
I have a bunch of changes and a crappy hard drive.
I had previously planned to only commit modules when they would compile,
    but I don't want to waste any space.
Also, holy fuck, there are a bunch of deep circular dependencies in the
    syntax for a constant-expression, so there are many pieces that
    cannot be factored into smaller modules.
I am going to commit the mess I've got so far.
I'll commit the AbstractSyntaxTree first, then the IntegerConstant module,
    I think.
I need to look in the AbstractSyntaxTree to find the changes I've made and
    the old code I commented out and delete that old code.
I'll do that now.
I committed the changes to the AbstractSyntaxTree and some changes to
    Identifier.hs, the PreprocessingParserX version.
The IntegerConstant module relies on the CustomCombinators module.
I will commit the CustomCombinators module first.
The IntegerConstant module, I believe, relies on changes to the
    PreprocessingParser module, as well.
I will commit those changes before committing the IntegerConstant module.

I have a couple issues and ideas that I thought of today at work that
    I would like to write down and explore.
The first thing that comes to mind is the idea of creating a typeclass
    with a function like show that would convert preprocessing tokens
    to strings.
It would be like the Show typeclass, but I want this to be different.
I may not even need a typeclass for this, but a function from an abstract
    data type PreprocessingToken -> String.
I think this could be a good idea because I want to be able to reuse all of
    the char token parsers used by the lexer.
Hopefully, the cabal/stack build system I create will help make it possible
    to reuse code in a clean way.
If I do decide to make the PreprocessingToken -> String function a
    typeclass, I could call it Unparse or ShowTokenString or something
    like those.
I think I like Unparse more.
It sounds cooler.
I might prefer ShowPPTokenString or ShowPreprocessingTokenString over
    ShowTokenString because the typeclass may not be reused by the compiler
    proper.
PreprocessingParserX can't be used for compiler proper.
Maybe that type can, but the type name cannot.
The compiler proper will probably have/need the same parsers as the
    preprocessor, but I do not know whether the compiler will need state
    during that phase/those phases.
When I have tried to plan far into the future on this project, my plans
    often have not worked or have not worked fast enough.
Even if the compiler proper needs the same parsers with different state, I
    may be able to create some parser transformers to create parsers that
    handle and use state as needed without having to rewrite boilerplate
    parsers.
On the topic of the PreprocessingToken -> String function, I think I would
    like to make it a typeclass.
I have never actually written a typeclass and do not know the syntax.
If for no other reason, I would like this to be a learning experience where
    I write a typeclass for the first time.
It is 8:28, almost time to get off the electronics.

It is May 03 today.
I feel like I am wasting a lot of my time implementing a bunch of typeclasses and shit.
So much of what I am doing seems like busy work.
I had planned on making a full implementation of a C preprocessor.
I think that will take longer than I want it to.
I think I am going to not implement a bunch of things, like universal character names.
I had planned on implementing them for the preprocessor, but not using them for the compiler proper.
However, I will not be using most of this syntax.
If I do want to use it in the future, I will implement it.
I think I will simply include a bunch of these syntaxes in alternatives separated by <|> combinator.
I will implement them with a parserFail message, though.
That way, the code shouldn't break, and I at least give a hint for myself as to where I left off.
I still think I might use the ASTEquipped module.
I might decide against that soon, though.
I am going to leave the mess I made for the HexQuad and the HexadecimalDigit types for now.
I just looked up how to do split screens in vim.
I am using vsplit and Ctrl+W and arrows to move around.
It is interesting.
I might grow to like this.
It is almost certainly less convenient than hitting Alt+Tab or Ctrl+Tab, but I think it is nice that the ordering doesn't change as if I were using different windows or was using Visual Studio.
I just realized that I can hit Ctrl+W twice in normal mode, and the cursor moves to the next window pane in vim.
That is pretty convenient.
I might use hexadecimal digits when I build the operating system.
However, for now I will not implement them.
I might have a parser built for them, but I do not want to handle supporting all the conversions needed between integers, characters, etc.

I wonder if I am overcomplicating a lot of the data types in the AST by deconstructing the strings.
For example, I am not storing identifiers as strings.
Rather, I am storing them as an IdentifierNonDigit and [IdentifierSuffix]
I just made that change.
I could go against that decision, but that would be inconsistent with my other ADTs in the AST.

I need to move the identifierNonDigit parser to its own module.
I will name the module IdentifierNonDigit.
The definition I will use is currently in the CharTokenParsers.PPNumbers.PPNumber module.
It is on line 67 at the moment.

I am going to refactor the Lexer.PreprocessingToken module now.
This is one of the highest level modules in the Preprocessor's Lexer, so this is a good sign to show that I am getting close to finishing the refactors to the Lexer (and all parsers) to use the AST data types.
Oh, shit!
I will still have to define and implement ShowLexer for all these data types! Fuck!
The code will be elegant, but holy fuck, it's going to be a lot of time, I think.
I hope it was a wise decision to write the code in this way.

I had to make changes to the AST and make the PreprocessingToken data type an actual AST and not just a String.
I implemented parsers based on other parsers for the PreprocessingToken data type.
I thought that would allow the Lexer.PreprocessingToken module to compile.
However, after I finished implementing the parsers for PreprocessingToken constructors, I saw some type matching errors that indicated that I still need to refactor the parsers that the PreprocessingToken constructor
    parsers are defined with.
That includes the headerName CharTokenParser.
I will refactor the CharTokenParsers.HeaderNames.HeaderName module next.
I will eventually swing back around to the Lexer.PreprocessingToken module.

I have reached a big design decision.
I will have to decide whether I will write code to deconstruct the Lexer's parse tree into strings.
Alternatively, I could write a wrapper that transforms the Char token parsers into parsers that return the
    consumed input and the parse tree/AST/lexed tokens.
In theory, I could write the later stages of the preprocessor to use parse the parse tree of lexed tokens, but I think that would create some code redundancy.
The later stages of the preprocessor might also not be as intuitive to write as they would be if I were writing a string parser.
I already ran into that issue a little bit, a couple months ago.
Some of the fine grained syntactical elements did not fill up an entire token sometimes, and I would have needed to have Char token parsers and String token parsers defined in the same module.
I would like to get away from that.
I tried to see if Parsec supports an easy way to get the input consumed by a parser.
It does not seem to have anything for that task out of the box.
I could probably call getInput before and after running the preprocessingToken parser, and compare the two to get the consumed input.
That could be a big performance hit.
I am not super concerned with the performance of this compiler currently.
I do not plan to use this compiler to compile greatly over 2000 lines of code at a time, if even.
That would be the operating system I eventually write.
I think I will ask stack overflow.
See https://stackoverflow.com/questions/76273112/how-do-i-get-the-input-consumed-by-a-parser-in-parsec
I am glad I asked that question.
I have not been very active on Stack Overflow lately.

I got a pretty good answer from stack overflow.
It was actually in a comment, not an answer per se.
I wrote a String version of stripSuffix (like stripSuffix in Data.Text of text package [see hackage])
I wrote stripSuffix as an internal function in the CustomCombinators module.
I used that to write a combinator parseADTAndConsumedInput.
It creates a parser that returns a tuple.
That tuple has 2 elements.
The first element is a list of the consumed tokens.\
The second element is the parsed ADT.
I was able to test the combinator with ghci (not using stack, just ghci CustomCombinators.hs)
I thought about testing more after I tested in ghci, specifically writing tests.
I might look into writing tests similar to those in the parsec github repository, except for my parsers.
I want to write regression tests, though, tests for which I can update the outputs.
I want an easy way to create tests AND update expected outputs for a given input.

I am going to try to use the parseADTAndConsumedInput combinator to make the lexer work as it did before I began the migration to char token parsers
    that parse ADTs.
How am I going to do that?
Well, I might need to create another data type in the Lexer that allows me to union the PreprocessingToken type and the String type.
The reason that I might need to union those types is because I want to handle the spacing (for the sake of good source position messages) parsed
    by the lexParser parser in the Lexer.Lexer module.
I could go back and fix that.
Well, that would affect the tokens parsed by the preprocessor proper.
Actually, I don't think it would.
I was going to have the lexer output a list of strings anyways.
I could use map.
It would be easy to make the necessary conversion from a product type of String and (String, PreprocessingToken) to String.
For each element in the list, I could select the String itself if the element was a String.
Otherwise, I could select the first element in the tuple using fst.
The ADT I define can be exclusively used in the Lexer.Lexer module.

I think I will eventually move the Preprocessor.IntegerConstant module to the CharTokenParsers set of modules.
I might do that some other time, but not now.

I am writing a parser constructor named t1SatisfyT2NoPrecedingWhiteSpace.
It is an abstraction of stringSatisfyTNoPrecedingWhiteSpace.
This is in the Preprocessor.PreprocessingParser module.
I am going to have to...
Wait, I just realized that I do not need to even write this parser constructor.
I will not need it in the preprocessor.
The lexer is going to return a [String] object, just as it used to before I began the migration to Char token ADT parsers.

I am confused.
Well, I need to clarify and structure my thoughts about transforming parsers from Char token ADT parsers into String token ADT parsers.
I determined that I will not need a t1SatisfyT2NoPrecedingWhiteSpace parser constructor.
I would like to note that I might need another parser constructor.
I also might need another parser transformer.
I may need both, and I may need more than one of one or both of them.
I might be able to get by cleanly without

I don't know what I was going to say.
I think I am going to write another parser transformer in the Preprocessor.PreprocessingParser module.
I think it will transform a Char token ADT parser to a String token ADT parser.
It will utilize the parseADTAndConsumedInput combinator and the stringSatisfyTNoPrecedingWhiteSpace parser constructor.
It will have to run the Char token parser and use its output.
I will probably utilize the stringParserToStringChecker function.
I will transform the Char token ADT parser to a (String, ADT) parser.
I will write a parser that takes that returns only the String in that tuple.
Then, I will utilize the Preprocessor.PreprocessingParser module's functions to transform that Char token String parser into a String token ADT parser.
Oh, wait!
I just realized that I cannot just transform the Char token String parser into a String token ADT parser.
I will need the ADT result from the same parse that gets me the String that I extracted from the tuple.
I will return the parsed ADT from the tuple.
I will probably only do that conditionally.
The condition would be that the consumed input is equal to the input token.
I could verify that easily and efficiently by using the eof parser from the parsec library.

I think I have an alternative to using parseADTAndConsumedInput.
I think that the reason I was going to use the parseADTAndConsumedInput combinator was because I need to make sure that a Chart token parser consumes the entire String that it is used to parse into
    an ADT.
I think that I can accomplish this task with the eof parser.
I just double checked, and the eof parser, defined in Text.Parsec.Combinator, works for parsers of any type of input stream.
I think that I can do a check to make sure that all Char tokens in a String were consumed by the parser to be transformed by using the eof parser.
I may have written a combinator like this previously.
I will probably git grep for "eof" to ensure that I have not.
I will do that now before writing more about my plans.
I have, in fact, written a combinator like the one that I just described.
It is in the CustomCombinators module.
It is named failsIfDoesNotConsumeAllInput.
That is nice.
However, I still need to figure out some more of the details.
I have not worked on this compiler for about 2 weeks.
I see the comment I have with an incomplete definition for t1SatisfyT2NoPrecedingWhiteSpace.
However, I do not remember all that I was trying to do with that function.
I guess I could benefit from reading the previous notes in this file.
I saw that I had notes saying that I do not need a t1SatisfyT2NoPrecedingWhiteSpace.
I was going to write a parser transformer instaead of writing a parser constructor that I would use on parsers that had been deconstructed into functions returning type Bool.
I will not be writing t1SatisfyT2NoPrecedingWhiteSpace.
I think it will be a generalization of stringParserSatisfyT or something like that.
I just noticed that I already have a parser transformer charToStringTokenParser.
I am going to check the git status and maybe run git diff to see if that is new.
That may be exactly what I need.
I may have already written the parser transformer that I need, and that might be it.
It is not new in source control.
It is already checked into source control.
That is odd.
I have confused myself.
I will see if that parser transformer is in use anywhere.
It is used in the Preprocessor.ConstantExpression module.
I am going to read through the definition of the charToStringTokenParser function again and determine if that is what I am going to use in the Preprocessor.PPTokens module.
I am now trying to write parsers to fill in some of the syntax in the Preprocessor.ConstantExpression module.
I might strip it all down to the bare bones and only allow a primaryExpression, and I might furthermore strip that down to only the constant or string literal variants.
I need to remember what this C compiler is for.
I only intend to use it for writing my operating system initially.
I doubt that I will need if directives that evaluate constant expressions.
I guess it depends if the preprocessor parsing the constant expressions, should the expression contain side effects like an assignment expression, maintain the altered state in later constant
    expressions.
I will probably have to read the constraints and semantics to discover whether this is the case or not.
I will do that now.
If there are no state changes allowed in the constant expressions, then I will probably simplify the implementation to fail due to lack of implementation for most of the syntax of
    constant expressions, as I outlined a few lines above.
I looked in the ISO document, and it says that some syntax is not allowed in a constant expression,
    such as assignment, increment, decrement, function-call, or comma operators, except when they
    contained within a subexpression that is not evaluated.
I am going to just implement the two morphs of primaryExpression that I mentioned earlier, constant and stringLiteral.
I will use the same AST as before, but most of the morphs of the syntaxes already defined (and those not defined yet) will fail with an error indicating that the syntax is not implemented yet.
Some of the syntax, like for logicalOrExpression, is implemented with the sepBy1 combinator, and when I write the code to handle the AST, I will ensure that it has only one element (i.e. no
    operators or parentheses or any other syntax beyond that of constant or stringLiteral are used).
I have noticed that I often get an error of the form "illegal term-level use of the type constructor" (with letter case not the same).
The error message goes on with several lines of information.
However, I have noticed that I get that error usually, if not always, when I have forgotten to import a data constructor, especially one of the same name as the data type or newtype.
I am going to create a single module that will define PreprocessingParserX parsers for the Constant subtree of the AST.
I do not think that I am going to support floating constants.
The linux kernel does not have any floats, so I will probably be okay without any in my operating system, even though I am writing more than just a kernel.

I am going to make this compiler even leaner.
I want a full pipeline up and running as soon as possible, at least given the effort that I put in.
I might simply plug the holes in the preprocessor, forget it for now, and move onto the compiler proper.
That will not erase the work I have done on/with the preprocessor.
I will be able to utilize many of the parsing primitives that I have refined.
I have just plugged up the the if directive.
By the if directive, I mean the directive that conditionally uses code based on a constant expression, not the ifdef nor ifndef directive.
I still agree with the idea of not supporting floats.
I am going to put the single module for the Constant subtree of the AST on hold for now, though.

I want to keep this project L E A N, lean.

I changed the name of the ConstantExpression constructor in the AST.
It is no longer ConditionalExpression.
It is now ConditionalExpressionConstantExpression.
I made this change before I wrote the notes in the previous two paragraphs.
I am just committing now.
I could remove the changes, but I would rather just commit them.

I now have a build that totally compiles.
Holy shit, this is nice.
However, the code I have written does not do hardly anything without a repl at this point in time.
I think I am going to move onto the compiler proper.
I can jump back to the preprocessor, but I think that I am mentally ready to take on the compiler proper, and working on the preprocessor seems like more of a time drain for now.
I do admit, though, for this compiler, I have not written any code dealing with code generation.
That could prove to be a tricky task that I may backtrack away from, or read more theory before going full speed ahead.
When writing code for the code generation phase of this compiler, it is likely that I will make changes to the assembler, as well.
Okay, where the hell am I going to jump into writing code for the compiler proper?
Well, I will probably do some architectural design now.
I will have to decide whether I actually want to use a lexer or not.
Parsing through the output of a lexer, if still split into tokens when input for syntactical analysis, can be error prone, as not all syntactical elements do not comprise a whole token.
Two examples of this are digit and non-digit (see the C ISO spec).
I just looked at the variants of the token syntactical construct (would be used in a lexer for the compiler proper).
It has 5 variants: keyword, identifier, constant, string-literal, and punctuator.
Keywords would be super straightforward and would probably take under 15 minutes.
It would mostly be a typing task.
I already have parsers for identifiers.
That would probably take under 10 minutes.

There is another thing that I need to take into account when determining whether or not I should require a lexer for the compiler proper: spacing.
Handling spacing is a very error prone task.
I have to ensure that spacing is allowed where necessary.
More importantly, there must not be leeway for spacing to be entered when it should not be allowed (according to the ISO standard).
I am not going to implement the C language entirely, but I want to make sure that this implementation does not accept invalid C code.
I want my implementation to accept a strict subset of the C programming language.
If I do not require a lexer and simply have the compiler proper parse through Strings, then the lexer is not a necessary piece of the pipeline.
I think I actually like that better.
I want to read about whether I should or should not use a lexer online before making a decision.
Everything I saw seems to recommend using a Lexer to output a stream of tokens.
I also realized that I probably will not be able to write a full compilation pipeline until I have the constant parser completed.
I might just finish that up and delay these design decisions.
I also think that I will need to change the output of the assembler to make something that can be linked and loaded by existing RISC-V tooling.
I moved the Preprocessor.IntegerConstant module to CharTokenParsers tree, as all of the parsers are Char token parsers.
I will do the same for the EnumerationConstant.
I just checked.
I do not have Char token parser code for the type EnumerationConstant.
There is only a definition of PreprocessingParserX EnumerationConstant.
All I need is a simpleExpression with an identifier parser.
I now have all of the variants of constant in the CharTokenParser tree, except floating-constant.
I am not implementing that, at least not any time soon.

I think that I will be able to write some compiler code now.
I am holding off from implementing the lexer just yet, though I think I will write code that expects lexed tokens.
I believe that the top level syntax construct in the C programming language is a translation-unit.
I want to confirm this before moving forward.
I confirmed by reading the semantics of a translation-unit and section 5.1.1.1 in the ISO document.
I will need a new AST.
Well, I will need new ADTs to create an AST with a different root.
I think I will store all of the ADTs in the AbstractSyntaxTree module that already exists, though.
I think that I have finished writing code for the AST.
Everything builds.

